\section{Properties of Probability Models}
The additivity property of probability measure automatically implies certain basic properties. These are true for
\emph{any} probability model at all.

If $A$ is any event, we write $A^c$ (read ``A complement'') for the event that $A$ does \emph{not} occur. In the
weather example, if $A=\{\text{rain}\}$, then $A^c=\{\text{snow, clear}\}$. In the coin examples, if $A$ is the event
that the first coin is heads, then $A^c$ is the event that first coin is tails.

Now, $A$ and $A^c$ are always disjoint. Furthermore, their union is always the entire sample space: $A \cup A^c = S$.
Hence, by the additivity property, we must have $P(A) + P(A^c) = P(S)$. But we always have $P(S)=1$. Thus, $P(A) +
P(A^c)=1$, or
\begin{equation}
    \label{eq:131}
    P(A^c)=1 - P(A)\,.
\end{equation}
In words, the probability that any event does \emph{not} occur is equal to one minus the probability that it
\emph{does} occur. This is a very helpful fact that we shall use often.

Now suppose that $A_1, A_2, \ldots$ are events that form a \term{partition} of the sample space $S$. This means that
$A_1, A_2, \ldots$ are disjoint and, furthermore, that their union is equal to $S$, i.e., $A_1 \cup A_2 \cup \ldots =
S$. We have the following basic theorem that allows us to decompose the calculation of the probability of $B$ into
the sum of the probabilities of the sets $A_i \cap B$. Often these are easier to compute.

\begin{theorem}[\term{Law of total probability}, \emph{unconditioned version}]
    Let $A_1, A_2, \ldots$ be events that form a partition of the sample space $S$. Let $B$ be any event. Then
    $$
    P(B) = P(A_1 \cap B) + P(A_2 \cap  B) + \ldots\,.
    $$
\end{theorem}
\begin{proof}
    The events $(A_1 \cap B)$, $(A_2\cap B)$, $\ldots$ are disjoint, and their union is $B$. Hence, the result
    follows immediately from the additivity property \autoref{eq:additive_rule}.
\end{proof}
A somewhat more useful version of the law of total probability, and applications of its use, are provided in
\autoref{cond_prob_indp}.

Suppose now that $A$ and $B$ are two events such that $A$ \emph{contains} $B$ (in symbols, $A \supseteq B$).
In words, all outcomes in $B$ are also in $A$. Intuitively, $A$ is ``large'' event than $B$, so we would expect its
probability to be larger.
We have the following result.
\begin{theorem}
    Let $A$ and $B$ be two events with $A \supseteq B$. Then
    \begin{equation}
        \label{eq:132}
        P(A) = P(B)+P(A \cap B^c)\,.
    \end{equation}
\end{theorem}
\begin{proof}
    We can write $A=B\cup (A \cap B^c)$, where $B$ and $A \cap B^c$ are disjoint. Hence, $P(A)=P(B)+P(A\cap B^c)$ by
    additivity.
\end{proof}
Because we always have $P(A \cap B^c)\geqslant 0$, we conclude the following.
\begin{corollary}[\term{Monotonicity}]
    Let $A$ and $B$ be two events, with $ A \supseteq B$. Then
    $$
    P(A) \leqslant P(B)\,.
    $$
\end{corollary}
On the other hand, rearranging \autoref{eq:132}, we obtain the following.
\begin{corollary}\label{cor:132}
    Let $A$ and $B$ be two events, with $A \supseteq B$. Then
    \begin{equation}
        P(A \cap B^c) = P(A) - P(B)\,.
    \end{equation}
\end{corollary}

More general, even if we do not have $A \supseteq B$, we have the following property.

\begin{theorem}[\term{Principle of inclusion-exclusion}, \emph{two-event version}]
    Let $A$ and $B$ be two events. Then
    \begin{equation}
        P(A \cup B) = P(A) + P(B) - P(A \cap B)\,.
    \end{equation}
\end{theorem}
\begin{proof}
    We can write $A\cup B = (A \cap B^c) \cup (B \cap A^c) \cup (A \cap B)$, where  $A\cap B^c$, $B \cap A^c$, and
    $A\cap B$ are disjoint. By additivity we have
    \begin{equation}\label{eq:prf_1}
        P(A\cup B) = P(A\cap B^c) + P(B \cap A^c)+P(A \cap B)\,.
    \end{equation}
    On the other hand, using \autoref{cor:132}(with $B$ replaced by $A \cap B$), we have
    \begin{equation}\label{eq:prf_2}
        P(A \cap B^c) = P(A \cap (A \cap B)^c) = P(A) - P(A \cap B)
    \end{equation}
    and similarly,
    \begin{equation}\label{eq:prf_3}
        P(B \cap A^c) = P(B) - P(A \cap B)\,.
    \end{equation}
    Substituting \autoref{eq:prf_2} and \autoref{eq:prf_3} into \autoref{eq:prf_1}, the result follows.
\end{proof}
A more general version of the principle of inclusion-exclusion is developed in \autoref{challenge:1310}.

Sometimes we do not need to evaluate the probability content of a union; we need only know it is bounded above by sum
of the probabilities of the individual events. This is called subadditivity.
\begin{theorem}[\term{Subadditivity}]
    Let $A_1, A_2,\ldots$ be a finite or countably infinite sequence of events, not necessarily disjoint. Then
    $$
    P(A_1\cup A_2\cup\ldots)\leqslant P(A_1)+P(A_2)+\ldots
    \,.
    $$
\end{theorem}
\begin{proof}
    See \autoref{ch2:adv_proofs} for the proof of this result.
\end{proof}

We note that some properties in the definition of a probability model actually follow from other properties. For
example, once we know the probability $P$ is additive and that $P(S)=1$, it follows that we \emph{must} have
$P(\emptyset)=0$. Indeed, because $S$ and $\empty$ are disjoint, $P(S\cup \emptyset)=P(S)+P(\emptyset)$. But of
course, $P(S \cup \emptyset) = P(S)=1$, so we must have $P(\emptyset)=0$.

Similarly, once we know $P$ is additive on countably infinite sequences of disjoint events, it follows that $P$ must
be additive on finite sequences of disjoint events, too. Indeed, given a finite disjoint sequence $A_1, \ldots, A_n$,
we can just set $A_i=\emptyset$ for all $i > n$, to get a countably infinite disjoint sequence with same union and
the same sum of probabilities.

\begin{summary}
    \item The probability of the complement of an event equals one minus the probability of the event.
    \item Probabilities always satisfy the basic properties of total probability, subadditivity, and monotonicity.
    \item The principle of inclusion-exclusion allows for the computation of $P(A \cup B)$ in terms of simpler events.
\end{summary}

\begin{exercises}
    \item Suppose $S=\{1,2,\ldots,100\}$. Suppose further that $P(\{1\})0.1$.
    \begin{enumerate}
        \item What is the probability $P(\{2,3,\ldots,100\})$?
        \item What is the smallest possible value of $P(\{1,2,3\})$?
    \end{enumerate}
    \item Suppose that Al watches the six o'clock news 2/3 of the time, watches the eleven o'clock news 1/2 of the
    time, and watches both the six o'clock and eleven o'clock news 1/3 of the time. For a randomly selected day, what
    is the probability that Al watches neither news?
    \item Suppose that an employee arrives late 10\% of the time, leaves early 20\% of the time, and both arrives
    late \emph{and} leaves early 5\% of the time. What is the probability that on a given day that employee will
    either arrive late \emph{or}  leave early (or both) ?
    \item Suppose your right knee is sore 15\% of the time, and your left knee is sore 10\% of the time. What is the
    largest possible percentage of time that at least one of your knees is sore? What is the smallest possible
    percentage of time that at least one of your knees is sore?
    \item Suppose a fair coin is flipped five times in a row.
    \begin{enumerate}
        \item What is the probability of getting all five heads?
        \item What is the probability of getting at least one tail?
    \end{enumerate}
    \item Suppose a card is chosen uniformly at random from a standard 52-card deck.
    \begin{enumerate}
        \item What is the probability that the card is a jack?
        \item What is the probability that the card is a club?
        \item What is the probability that the card is both a jack and a club?
        \item What is the probability that the card is either a jack or a club (or both)?
    \end{enumerate}
    \item Suppose your team has a 40\% chance of winning or tying today's game and has a 30\% chance of winning
    today's game. What is the probability that today's game will be a tie?
    \item Suppose 55\% of students are female, of which 4/5(44\%) have long hair, and 45\% are male, of which 1/3
    (15\% of all students) have long hair. What is the probability that a student chosen at random will either be
    female or have long hair (or both)?
\end{exercises}

\begin{problems}
    \item Suppose we choose a positive integer at random, according to some unknown probability distribution. Suppose
    we know that $P(\{1,2,3,4,5\})=0.3$, that $P({4,5,6})=0.4$, and that $P(\{1\})=0.1$. What are the largest and
    smallest possible values of $P(\{2\})$?
\end{problems}

\begin{challeges}
    \item     \label{challenge:1310} Generalize the principle of inclusion-exclusion, as follows.
    \begin{enumerate}
        \item Suppose there are three events $A$, $B$, and $C$. Prove that
        \begin{equation*}
            P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B\cap C)+ P(A \cap B \cap
            C)\,.
        \end{equation*}
        \item Suppose there are $n$ events $A_1$, $A_2$, $\ldots$ ,$A_n$. Prove that
        \begin{equation*}
            \begin{split}
                P(A_1 \cup \cdots \cup A_n) &= \sum_{i=1}^{n}P(A_i) - \sum_{\substack{i,j=1 \\ i<j}}^{n}P(A_i
                \cap
                A_j) +
                \sum_{\substack{i,j,k=1 \\ i < j < k}}^{n}P(A_i\cap A_j \cap A_k) \\
                &- \ldots \pm P(A_1 \cap \cdots \cap A_n)\,.
            \end{split}
        \end{equation*}
    \end{enumerate}
    (Hint: Use induction.)
\end{challeges}

\begin{discussion}
    \item Of the various theorems presented in this section, which ones do you think are the most important? Which
    ones do you think are the least important? Explain the reasons for your choices.
\end{discussion}
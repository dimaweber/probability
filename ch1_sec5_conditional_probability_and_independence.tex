\section{Conditional Probability and Independence}\label{cond_prob_indp}
Consider again the three-coin example as in \autoref{example:143}, where we flip three different fair coins, and
$$
S=\{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\}\,,
$$
with $P(s)=1/8$ for each $s \in S$. What is the probability that the first coin comes up heads? Well, of  course , this should be 1/2. We can see this more formally by saying that $P(\text{first coin heads})=P(\{HHH, HHT, HTH, HTT\})=4/8=1/2$, as it should.

But suppose  now that an informant tell us than exactly two of the three coins came up heads. \emph{Now} what is the probability that the first coin was heads?

The point is that this informant has changed our available information, i.e., changed our level of ignorance. It follows that our corresponding probabilities should also change. Indeed, if we know that exactly two of coins were heads, then we know that the outcome was one of $HHT$, $HTH$, and $THH$. Because those three outcomes should (in this case) still all be equally likely, and because only the first two correspond to the first coin being heads, we conclude the following: if we know that exactly two of the three coins are heads, \emph{then} the probability that the first coin is heads is 2/3.

More precisely, we have computed a \term{conditional probability}. That is, we have determined that \emph{conditional} on knowing that exactly two coins came up heads, the \emph{conditional probability} of the first coin being heads is 2/3. We write this in mathematical notation as
$$
P(\text{first coin heads} \mid \text{two coins heads}) = 2/3\,.
$$
Here the vertical bar $\mid$ stands for ``conditional on'', or ``given that''.

\subsection{Conditional Probability}
In general, given two events $A$ and $B$ with $P(B)>0$, the \term{conditional probability} of $A$ given $B$, written $P(A\mid B)$, stands for the fraction of the time that $A$ occurs once we \emph{know} that $B$ occurs. It is computed as the ratio of the probability that $A$ and $B$ \emph{both} occur, divided by the probability that $B$ occurs, as follows.

\begin{definition}[Conditional probability]
    Given two events $A$ and $B$, with $P(B)>0$, the conditional probability of $A$ given $B$ is equal to
    \begin{equation}\label{eq:conditional_probability_definition}
        P(A\mid B) = \frac{P(A \cap B)}{P(B)}\,.
    \end{equation}
\end{definition}

The motivation for \autoref{eq:conditional_probability_definition} is as follow. The event $B$ will occur a fraction $P(B)$ of the time. Also, \emph{both} $A$ and $B$ will occur a fraction $P(A \cap B)$ of the time. The ratio $P(A \cap B) / P(B)$ thus gives the \emph{proportion} of the times when $B$ occurs, that $A$ \emph{also} occurs. That is, if we \emph{ignore} all the times that $B$ does not occur and consider only those times that $B$ does occur, then the ratio $P(A \cap B)/P(B)$ equals the fraction of the time that $A$ will also occur. This is precisely what is meant by the conditional probability of $A$ given $B$.

In the example just computed, $A$ is the event that the first coin is heads, while $B$ is the event that exactly two coins were heads. Hence, in mathematical terms, $A=\{HHH, HHT, HTH, HTT\}$ and $B=\{HHT, HTH, THH\}$. It follows that $A \cap B=\{HHT, HTH\}$. Therefore
$$
    P(A \mid B) = \frac{P(A\cap B)}{P(B)} = \frac{P(\{HHT, HTH\})}{P(\{HHT, HTH, THH\})} = \frac{2/8}{3/8}=2/3\,,
$$
as already computed.

On the other hand, we similarly compute that
$$
    P(\text{first coin tails} \mid \text{two coins heads}) =  1/3\,.
$$
We thus see that conditioning on some event (such as ``two coins heads'') can make probability either increase (as for event ``first coin heads'') or decrease (as for the event ``first coin tails'').

The definition of $P(A \mid B)$ immediately leads to the \term{multiplication formula}
\begin{equation}
    \label{eq:152}
    P(A \cap B) = P(A)\cdot P(B \mid A)\,.
\end{equation}
This allows us to compute the joint probability of $A$ and $B$ when we are given the probability of $A$ and the conditional probability of $B$ given $A$.

Conditional probability allows us to express \autoref{theorem:law_of_total_probability_unconditional_version}, the law of total probability. in a different and sometimes more helpful way.
\begin{theorem}[Law of total probability, conditioned version]
    \label{theorem:law_of_total_probability_conditioned_version}
    Let $A_1, A_2,\ldots$, be events that form a partition of the sample space $S$, each of positive probability. Let $B$ be any event. Then
    $$
        P(B)=P(A_1)\cdot P(B\mid A_1)+P(A_2)\cdot P(B\mid A_2)+\ldots\,.
    $$
\end{theorem}
\begin{proof}
    The multiplication \autoref{eq:152} gives that $$P(A_i\cap B) = P(A_i)\cdot P(B\mid A_i)\,.$$ The result then follows immediately from \autoref{theorem:law_of_total_probability_unconditional_version}.
\end{proof}

\begin{example}
    Suppose a class contains 60\% girls and 40\% boys. Suppose that 30\% of the girls have long hair, and 20\% of the boys have long hair. A student is chosen uniformly at random from the class. What is the probability that the chosen student will have long hair?

    To answer this, we let $A_1$ be the set of girls and $A_2$ be the set of boys. Then $\{A_1, A_2\}$ is a partition of the class. We further let $B$ be the set of all students with long hair.

    We are interested in $P(B)$. We compute this by \autoref{theorem:law_of_total_probability_conditioned_version} as
    $$
        P(B)=P(A_1)\cdot P(B\mid A_1) + P(A_2)\cdot P(B\mid A_2) = 0.6 \cdot 0.3 + 0.4 \cdot 0.2 = 0.26\,,
    $$
    so there is a 26\% chance that the randomly chosen student has long hair.
\end{example}

Suppose now that $A$ and $B$ are two events, each of positive probability. In some applications, we are given the values $P(A)$, $P(B)$, and $P(B\mid A)$ and want to compute $P(A\mid B)$. The following result establishes a simple relationship among these quantities.

\begin{theorem}[Bayes' theorem]\label{theorem:bayes_theorem}
    Let $A$ and $B$ be two events, each of positive probability. Then
    $$
        P(A \mid{} B) = \frac{P(A)}{P(B)}\cdot P(B \mid{} A)\,.
    $$
\end{theorem}
\begin{proof}
    We compute that
    $$
        \frac{P(A)}{P(B)}\cdot{}P(B\mid{}A)=\frac{P(A)}{P(B)}\cdot\frac{P(A\cap{}B)}{P(A)}=\frac{P(A\cap{}B)}{P(B)}=P(A \mid B)\,.
    $$
    This gives the result.
\end{proof}
Standard application of the multiplication formula, the law of total probabilities, and Bayes' theorem occur with \term{two-stage systems}. The response for such systems can be thought of as occurring in two steps or stages. Typically, we are given the probabilities for the first  stage and the conditional probabilities for the second stage. The multiplication formula is then  used to calculate joint probabilities for what happens at both stages; the law of total probability is used to compute the probabilities for what happens at the second stage; and Bayes' theorem is  used to calculate the conditional probabilities for the first stage, given what has occurred at the second stage. We illustrate this by an example.

\begin{example}
    Suppose urn \#1 has 3 red and 2 blue balls, and urn \#2 has 4 red and 7 blue balls. Suppose one of the two urns is selected with probability 1/2 each, and then one of the balls within that urn is picked uniformly at random.

    What is the probability that urn \#2 is selected at the first stage (event $A$) and a blue ball is selected at the second stage (event $B$)? The multiplication formula provides the correct way to compute this probability as
    $$
        P(A \cap B) = P(A)\cdot P(B \mid A)=\frac{1}{2} \cdot \frac{7}{11} = \frac{7}{22}\approx0.318\,.
    $$

    Suppose instead we want to compute the probability that a blue ball is obtained. Using the law of total probability (\autoref{theorem:law_of_total_probability_conditioned_version}), we have that
    $$
        P(B)=P(A)\cdot{}P(B\mid{}A)+P(A^c)\cdot{}P(B\mid{}A^c)=\frac{1}{2}\cdot\frac{2}{5}+\frac{1}{2}\cdot\frac{7}{11}=\frac{22}{110}+\frac{35}{110}=\frac{57}{110}\approx0.518\,.
    $$

    Now suppose we are given the information that the ball picked is blue. Then, using Bayes' theorem, the conditional probability that we had selected urn \#2 is given by
    $$
        P(A\mid B)=\frac{A}{B}\cdot P(B\mid A)= \frac {\frac{1}{2}} {\frac{1}{2}\cdot\frac{2}{5}+\frac{1}{2}\cdot\frac{7}{11}} \cdot \frac{7}{11} = \frac{35}{57}\approx0.614\,.
    $$

    Note that, without the information that a blue ball occurred at the second stage, we have that $P(\text{urn \#2 selected})=1/2$\,. We see that knowing the ball was blue significantly \emph{increases} the probability that urn \#2 was selected.
\end{example}
We can represent a two-stage system using a tree, as in \autoref{fig:151}. It can be helpful to draw such a figure when carrying out probability computations for such systems. There are two possible outcomes at the first stage and three possible outcomes at the second stage.
\begin{figure}[h]
\digraph[scale=0.6]{fig151}{
    #splines=ortho;
    layout=dot;
    node [shape=box];
    S -> fs1;
    S -> fs2;
    fs1 -> ss1;
    fs1 -> ss2;
    fs1 -> ss3;
    fs2 -> ss1a;
    fs2 -> ss2a;
    fs2 -> ss3a;
    S [label=S]
    fs1 [label="first stage\-outcome 1"];
    fs2 [label="first stage\-outcome 2"];
    ss1 [label="second stage\-outcome 1"];
    ss2 [label="second stage\-outcome 2"];
    ss3 [label="second stage\-outcome 3"];
    ss1a [label="second stage\-outcome 1"];
    ss2a [label="second stage\-outcome 2"];
    ss3a [label="second stage\-outcome 3"];
}
\caption{A tree depicting a two-stage system with two possible outcomes at the first stage and three possible outcomes at the second stage.\label{fig:151}}
\end{figure}

\subsection{Independence of Events}\label{independence_of_events}
Consider now \autoref{example:144}, where we roll one fair die and flip one fair coin, so that
$$
S=\{1H, 2H, 3H, 4H, 5H, 6H, 1T, 2T, 3T, 4T, 5T, 6T\}
$$
and $\forall s\in S:P(\{s\})=1/12$. Here the probability that the die comes up 5 is equal to $P(\{5H,5T\})=2/12=1/6$, as it should be.

But now, what if the probability that die comes 5, \emph{conditional} on knowing that the coin came up tails? Well, we can compute that probability as
\begin{equation*}
    \begin{split}
P(\text{die = 5}\mid\text{coin = tails}) &= \frac{P(\text{die = 5 and coin = tails})}{P(\text{coin = tails})} \\
                                         & = \frac{P(\{5T\})}{P(\{1T,2T,3T,4T,5T,6T\})}= \\
                                         &=\frac{1/12}{6/12}=\frac1 6\,.
    \end{split}
\end{equation*}
This is the same as the unconditional probability, $P(\text{die = 5})$. It seems that knowing that the coin was tail had no effect on the probability that the coin came up 5. This property called \term{independence}. We say that the coin and the die are \emph{independent} in this example, to indicate that the occurrence of one does not have any influence on the probability ot the other occurrence.

More formally, we make the following definition.

\begin{definition}\label{def:152}
    Two events are independent if
    $$
        P(A\cap B) = P(A) \cdot P(B)\,.
    $$
\end{definition}
Now, because $P(A\mid B)=P(A\cap B)/P(B)$, we see that $A$ and $B$ are independent if and only if $P(A\mid B)=P(A)$ or $P(B\mid A)=P(B)$, provided that $P(A)>0$ and $P(B)>0$. Definition \autoref{def:152} has the advantage that it remains valid even if $P(B)=0$ or $P(A)=0$, respectively. Intuitively, events $A$ and $B$ are independent if neither one has any impact in the probability of the other.

\begin{example}
    In \autoref{example:144}, if $A$ is the event that the die was 5, and $B$ is the event that the coin was tails, then
    $$
    \begin{aligned}
        P(A) &= P(\{5H,5T\}) &=2/12 =1/6  \\
        P(B) &= P(\{1T,2T,3T,4T,5T,6T\}) &= 6/12 = 1/2
    \end{aligned}
    $$
    Also, $P(A \cap B) = P(\{5T\})=1/12$, which is indeed equal to $\dfrac16\cdot\dfrac12$. Hence, $A$ and $B$ are independent in this case.
\end{example}
For multiple events, the definition of independence is somewhat more involved.
\begin{definition}    \label{def:153}
    A collection of events $A_1, A_2, A_3,\ldots$ are \emph{independent} if
    $$
        P(A_{i_1}\cap\dots\cap A_{i_j}) = P(A_{i_1})\cdot\ldots\cdot P(A_{i_j})
    $$
    for \emph{any} finite subcollection $A_{i_1},\dots,A_{i_j}$ of distinct events.
\end{definition}
\begin{example}
    According to \autoref{def:153}, three events, $A$, $B$, and $C$ are independent if \emph{all} of the following equations hold:
    \begin{align}
        \begin{split}
            \label{eq:153}
            P(A \cap B) &= P(A)\cdot P(B)\,, \\
            P(A \cap C) &= P(A)\cdot P(C)\,, \\
            P(B \cap C) &= P(B) \cdot P(C)\,,
        \end{split}
    \intertext{and}
    P(A \cap B \cap C)     &= P(A)\cdot P(B) \cdot P(C)\,.\label{eq:154}
    \end{align}

    It is not sufficient to check just \emph{some} of these conditions to verify independence. For example, suppose that $S=\{1,2,3,4\}$, with $P(\{1\})=P(\{2\})=P(\{3\})=P(\{4\})=1/4$. Let $A=\{1,2\}$, $B=\{1,3\}$, and $C=\{1,4\}$. Then each of the three equations \eqref{eq:153} holds, but \autoref{eq:154} does not hold. Here, the events $A$, $B$, and $C$ are called \term{pairwise independent}, but they are not independent.
\end{example}

\begin{summary}
    \item Conditional probability measures the probability that $A$ occurs given that $B$ occurs; it it given by $P(A\mid B)=P(A \cap B) / P(B)$.
    \item Conditional probability satisfies its own law of total probability.
    \item Events are independent if they have no effect on each other's probabilities. Formally, this means that $P(A \cap B)=P(A)\cdot P(B)$.
    \item If $A$ and $B$ are independent, and $P(A)>0$ and $P(B)>0$, then $P(A\mid B)=P(A)$ and $P(B\mid A)=P(B)$.
\end{summary}

\begin{exercises}
    \item Suppose that we roll four fair six-sided dice.
        \begin{enumerate}
            \item What is conditional probability that the first die shows 2, conditional on the event that exactly three dice show 2?
            \item What is conditional probability that the first die shows 2, conditional on the event that \emph{at least} three dice show 2?
        \end{enumerate}
    \item Suppose we flip two fair coins and roll one fair six-sided die.
        \begin{enumerate}
            \item What is probability that the number of heads equals the number showing on the die?\label{it:a}
            \item What is conditional probability that the number of heads equals the number showing on the die, conditional on knowing that the die showed 1?\label{it:b}
            \item Is the answer for \autoref{it:b} larger or smaller than the answer for \autoref{it:a}? Explain intuitively why this is so.
        \end{enumerate}
    \item Suppose we flip three fair coins
        \begin{enumerate}
            \item What is the probability that all three coins are heads?
            \item What is the conditional probability  that all three coins are heads, conditional on knowing that the number of heads is odd?
            \item What is the conditional probability that all three coins are heads, given that the number of heads is even?
        \end{enumerate}
    \item Suppose we deal five cards from an ordinary 53-card deck. What is the conditional probability that all five cards are spades, given that at least four of them are spades?
    \item Suppose we deal five cards from an ordinary 52-card deck. What is the conditional probability that the hand contains all four aces, given that the hand contains at least four aces?
    \item Suppose we deal five cards from an ordinary 52-card deck. What is the conditional probability that the hand contains no pairs, given that it contains no spades?
    \item Suppose a baseball pitcher throws fastballs 80\% of the time and curveballs 20\% of the time. Suppose a batter hits run on 8\% of all fastball pitches, and on 5\% of all curveball pitches. What is the probability that this batter will hit a home run on this pitcher's next pitch?
    \item Suppose the probability of snow is 20\%, and the probability of a traffic accident is 10\%. Suppose further that the \emph{conditional} probability of an accident, given that it snows, is 40\%. What is the conditional probability that it snows, given that there is an accident?
    \item Suppose we roll two fair six-sided dice, one red and one blue. Let $A$ be the event that the two dice show the same value. Let $B$ be the event that the sum of the two dice is equal to 12. Let $C$ be the event that the red die shows 4. Let $D$ be the event that the blue die shows 4.
        \begin{enumerate}
            \item Are $A$ and $B$ independent?
            \item Are $A$ and $C$ independent?
            \item Are $A$ and $D$ independent?
            \item Are $C$ and $D$ independent?
            \item Are $A$, $C$, and $D$ all independent?
        \end{enumerate}
    \item Consider two urns, labeled urn \#1 and urn \#2. Suppose, as in \autoref{exercise:1411}, that urn \#1 has 5 red and 7 blue balls, that urn \#2 has 6 red and 12 blue balls, and that we pick three balls uniformly at random from each of the two urns. Conditional on the fact that all six chosen balls are the same  color, what is the conditional probability that this color is red?
    \item Suppose we roll a fair six-sided die and ten flip a number of fair coins equal to the number showing on the die. (For example, if the die shows 4, then we flip 4 coins.)
        \begin{enumerate}
            \item What is the probability that the number of heads equals 3?
            \item Conditional on knowing that the number of heads equals 3, what is the conditional probability that the die showed the  number 5?
        \end{enumerate}
    \item Suppose we roll a fair six-sided die and then pick a number of cards from a well-shuffled deck equal to the number showing on the die. (For example, if the die shows 4, then we pick 4 cards.)
        \begin{enumerate}
            \item What is the probability that the number of jacks in our hand equals 2?
            \item Conditional on knowing that the number of jacks in our hand equals 2, what is the conditional probability that the die showed the number 3?
        \end{enumerate}
\end{exercises}

\begin{problems}
    \item Consider three cards, as follows: One is red on both sides, one is black on both sides, and one is red on one side and black in the other. Suppose the cards are placed in a hat, and one is chosen ab random. Suppose further that this card is placed flat on the table, so we can see one side only.
        \begin{enumerate}
            \item What is the probability that this one side is red?
            \item \label{problem:1513b}Conditional on this one side being red, what is the probability that the card showing is the one that is red on both sides? (Hint: The answer is somewhat surprising.)
            \item Suppose you wanted to verify the answer in  \autoref{problem:1513b}, using an actual, physical experiment. Explain how you could do this.
        \end{enumerate}
    \item Prove that $A$ and $B$ are independent if and only if $A^c$ and $B$ are independent.
    \item Let $A$ and $B$ be events of positive probability. Prove that $P(A\mid B) > P(A)$ if and only if $P(B\mid A)>P(B)$.
\end{problems}

\begin{challeges}
    \item Suppose we roll three fair six-sided dice. Compute the conditional probability that the first die shows 4, given that the sum of the three numbers showing is 12.
    \item (\term{The game of craps}) The game of craps is played by rolling two fair, six-sided dice. On the first roll, if the sum of the two numbers showing equals 2, 3, or 12, then the player immediately loses. If the sum equals 7 or 11, then the player immediately wins. If the sum equals any other value, then this value becomes the player's ``point''. The player then repeatedly rolls the two dice, until such time as he or she either rolls the point value again (in which case he or she wins) or rolls a 7 (in which case he or she loses).
        \begin{enumerate}
            \item \label{challenge:1517a}Suppose the player's point is equal to 4. Conditional on this, what is the conditional probability that he or she will win(i.e., will roll another 4 before rolling a 7)? (Hint: The final roll will be either a 4 or 7; what is the conditional probability that it is 4?)
            \item \label{challenge:1517b}For $2\leqslant i \leqslant 12$, let $p_i$ be the conditional probability that the player will win, conditional on having rolled $i$ in the first roll. Compute $p_i$ for all $i: 2 \leqslant i \leqslant 12$. (Hint: You've already done this for $i=4$ in \autoref{challenge:1517a}. Also, the cases $i=2,3,7,11,12$ are trivial. The other cases are similar to the $i=4$ case.)
            \item Compute the overall probability that a player will win at craps. (Hint: Use \autoref{challenge:1517b} and \autoref{theorem:law_of_total_probability_conditioned_version}.)
        \end{enumerate}
    \item \label{challenge:1518}(\term{The Monty Hall problem}) Suppose there are three doors, labeled $A$, $B$, and $C$. A new car is behind one of the three doors, but you don't know which. You select one of the doors, say, door $A$. The host then opens one of doors $B$ or $C$, as follows: If the car is behind $B$, then they open  $C$; if the car is behind $C$, then they open $B$; if the car is behind $A$, then they open either $B$ or $C$ with probability 1/2 each. (In any case, the door opened by the host will \emph{not} have the car behind it.) The host then gives you the \emph{option} of either sticking with your original door choice (i.e., $A$), or switching to the remaining unopened door (i.e., whichever of $B$ or $C$ the host did not open). You then win (i.e., get to keep the car) if and only if the car is behind your final door selection. (\cite{B2}.) Suppose for definiteness that the host opens door $B$.
    \begin{enumerate}
       \item \label{challenge:1518a}If you stick with your original choice (i.e., door $A$), conditional on the host having opened door $B$, then what is your probability of winning? (Hint: First condition on the true location of the car. Then use \autoref{theorem:bayes_theorem})
       \item \label{challenge:1518b}If you switch to the remaining door (i.e., door $C$), conditional on the host having opened door $B$, then what is your probability of winning?
       \item Do you find the result of parts \autoref{challenge:1518a} and \autoref{challenge:1518b} surprising? How could you design a physical experiment to verify the result?
       \item Suppose we changed the rules so than, if you originally chose $A$ and the car was indeed behind $A$, then the host always opens door $B$. How would the answers to parts \autoref{challenge:1518a} and \autoref{challenge:1518b} change in this case?
       \item \label{challenge:1518e}Suppose we change the rules so that, if you originally chose $A$, then the host always opens door $B$ \emph{no matter where the car is}. We then \emph{condition} on the fact that door $B$ happened \emph{not} to have a car behind it. How would the answers to parts \autoref{challenge:1518a} and \autoref{challenge:1518b} change in this case?
   \end{enumerate}
\end{challeges}


\begin{discussion}
    \item Suppose two people each flip a fair coin simultaneously. Will the results of the two flips usually be independent? Under what sorts of circumstances might they not be independent? (List as many such circumstances as you can.)
    \item Suppose you are able to repeat an experiment many times, and you wish to check whether or not two events are independent. How might you go about this?
    \item The Monty Hall problem (\autoref{challenge:1518}) was originally presented by Marilyn von Savant, writing in the ``Ask Marilyn'' column of \emph{Parade Magazine}. She gave the correct answer. However, many people (including some well-known mathematicians, plus many laypeople) wrote in to complain that her answer was incorrect. The controversy dragged on for months, with many letters and very strong language written by both sides (in the end, von Savan was vindicated). Part of the confusion lay in the assumptions being made, e.g., some people misinterpreted her question as that of the modified version of \autoref{challenge:1518e}. However, a lot of the confusion was simply due to mathematical errors and misunderstandings. (Source: \emph{Parade Magazine}, ``Ask Marilyn'' column, September 9, 1990; December 2, 1990; February 17, 1991; July 7, 1991.)
    \begin{enumerate}
        \item Does it surprise you that so many people, including well-known mathematicians, made errors in  solving this problem? Why or why not?
        \item Does it surprise you that so many people, including many laypeople, cared so strongly about the answer to this problem? Why or why not?
    \end{enumerate}
\end{discussion}
